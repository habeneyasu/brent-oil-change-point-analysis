"""
Task 2: Change Point Modeling and Insight Generation

Core Analysis: Data Preparation, EDA, and Bayesian Change Point Detection
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Add src to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from data_processing.load_data import BrentOilDataLoader
from data_processing.preprocess import TimeSeriesAnalyzer
from visualization.plots import TimeSeriesVisualizer
from models.change_point_model import BayesianChangePointModel
from utils.config import setup_logging

# Setup logging
logger = setup_logging(log_level="INFO")

# Set visualization style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (14, 6)
plt.rcParams['font.size'] = 10


def main():
    """Main execution function for Task 2 Core Analysis."""
    logger.info("=" * 80)
    logger.info("Task 2: Change Point Modeling and Insight Generation")
    logger.info("Core Analysis: Data Preparation and EDA")
    logger.info("=" * 80)
    
    # ========================================================================
    # 1. Data Preparation and EDA
    # ========================================================================
    logger.info("\n" + "=" * 80)
    logger.info("STEP 1: Data Preparation and EDA")
    logger.info("=" * 80)
    
    # Load data
    logger.info("\n1.1 Loading data...")
    loader = BrentOilDataLoader()
    data = loader.load()
    
    # Convert Date to datetime (already done in loader, but verify)
    if not pd.api.types.is_datetime64_any_dtype(data['Date']):
        data['Date'] = pd.to_datetime(data['Date'])
    
    logger.info(f"   âœ… Loaded {len(data):,} records")
    logger.info(f"   Date range: {data['Date'].min()} to {data['Date'].max()}")
    logger.info(f"   Price range: ${data['Price'].min():.2f} - ${data['Price'].max():.2f}")
    
    # Plot raw Price series
    logger.info("\n1.2 Plotting raw Price series...")
    visualizer = TimeSeriesVisualizer()
    data_indexed = data.set_index('Date')
    
    fig = visualizer.plot_price_series(
        data_indexed,
        title="Brent Oil Price Time Series (1987-2022) - Raw Data"
    )
    plt.close(fig)  # Close to free memory
    logger.info("   âœ… Price series plot saved to reports/price_series.png")
    
    # Visual analysis notes
    logger.info("\n   ðŸ“Š Visual Analysis:")
    logger.info("      - Overall upward trend from 1987 to 2022")
    logger.info("      - Major price spikes visible (2008, 2011, 2022)")
    logger.info("      - Periods of high volatility (2008-2009, 2014-2016, 2020)")
    logger.info("      - Significant price drops (1998, 2008, 2014, 2020)")
    
    # ========================================================================
    # 2. Log Returns Analysis
    # ========================================================================
    logger.info("\n" + "=" * 80)
    logger.info("STEP 2: Log Returns Analysis")
    logger.info("=" * 80)
    
    # Compute log returns
    logger.info("\n2.1 Computing log returns...")
    returns = loader.compute_returns(method='log')
    
    logger.info(f"   âœ… Computed {len(returns):,} log returns")
    logger.info(f"   Mean return: {returns.mean():.6f}")
    logger.info(f"   Std return: {returns.std():.6f}")
    logger.info(f"   Annualized volatility: {returns.std() * np.sqrt(252):.2%}")
    
    # Plot log returns
    logger.info("\n2.2 Plotting log returns...")
    returns_indexed = returns.to_frame('Returns')
    returns_indexed.index = data_indexed.index[1:]  # Align with dates
    
    fig = visualizer.plot_returns(
        returns_indexed['Returns'],
        title="Log Returns Time Series - Volatility Clustering Analysis"
    )
    plt.close(fig)
    logger.info("   âœ… Log returns plot saved to reports/returns_series.png")
    
    # Analyze volatility clustering
    logger.info("\n2.3 Analyzing volatility clustering...")
    analyzer = TimeSeriesAnalyzer(data)
    volatility_results = analyzer.analyze_volatility(window=30)
    
    logger.info(f"   Mean volatility: {volatility_results['mean_volatility']:.6f}")
    logger.info(f"   Annualized volatility: {volatility_results['annualized_volatility']:.2%}")
    
    if 'interpretation' in volatility_results['volatility_clustering']:
        logger.info(
            f"   Volatility clustering: "
            f"{volatility_results['volatility_clustering']['interpretation']}"
        )
    
    # Plot volatility
    logger.info("\n2.4 Plotting rolling volatility...")
    fig = visualizer.plot_volatility(
        volatility_results['rolling_volatility'],
        title="Rolling Volatility (30-day window) - Volatility Clustering"
    )
    plt.close(fig)
    logger.info("   âœ… Volatility plot saved to reports/volatility_series.png")
    
    # Volatility clustering observations
    logger.info("\n   ðŸ“Š Volatility Clustering Observations:")
    logger.info("      - High volatility periods cluster together")
    logger.info("      - Crisis periods show elevated volatility (2008, 2014, 2020)")
    logger.info("      - Calm periods show low volatility (1990s, early 2000s)")
    logger.info("      - This pattern supports the use of change point models")
    
    # ========================================================================
    # 3. Stationarity Testing
    # ========================================================================
    logger.info("\n" + "=" * 80)
    logger.info("STEP 3: Stationarity Testing")
    logger.info("=" * 80)
    
    logger.info("\n3.1 Testing stationarity of prices...")
    adf_results = analyzer.test_stationarity(method='adf')
    logger.info(f"   ADF test: {adf_results['interpretation']} (p={adf_results['p_value']:.4e})")
    
    kpss_results = analyzer.test_stationarity(method='kpss')
    logger.info(f"   KPSS test: {kpss_results['interpretation']} (p={kpss_results['p_value']:.4e})")
    
    logger.info("\n3.2 Testing stationarity of log returns...")
    returns_analyzer = TimeSeriesAnalyzer(
        pd.DataFrame({'Date': returns_indexed.index, 'Price': returns_indexed['Returns']})
    )
    returns_adf = returns_analyzer.test_stationarity(method='adf')
    logger.info(f"   Returns ADF test: {returns_adf['interpretation']} (p={returns_adf['p_value']:.4e})")
    
    logger.info("\n   ðŸ“Š Stationarity Conclusion:")
    logger.info("      - Price series: Non-stationary (has trend)")
    logger.info("      - Log returns: More stationary (suitable for change point modeling)")
    logger.info("      - Change point model will detect structural breaks in mean/volatility")
    
    # ========================================================================
    # 4. Summary and Next Steps
    # ========================================================================
    logger.info("\n" + "=" * 80)
    logger.info("CORE ANALYSIS SUMMARY")
    logger.info("=" * 80)
    
    logger.info("\nâœ… Completed:")
    logger.info("   1. Data loaded and Date column converted to datetime")
    logger.info("   2. Raw Price series plotted - major trends and shocks identified")
    logger.info("   3. Log returns computed and plotted - volatility clustering observed")
    logger.info("   4. Rolling volatility analyzed - clustering patterns confirmed")
    logger.info("   5. Stationarity tests performed - returns suitable for modeling")
    
    logger.info("\nðŸ“Š Key Findings:")
    logger.info(f"   - {len(data):,} daily observations from {data['Date'].min().year} to {data['Date'].max().year}")
    logger.info(f"   - Price range: ${data['Price'].min():.2f} to ${data['Price'].max():.2f}")
    logger.info(f"   - Annualized volatility: {volatility_results['annualized_volatility']:.2%}")
    logger.info("   - Clear evidence of volatility clustering")
    logger.info("   - Multiple structural breaks visible in price series")
    
    logger.info("\nðŸŽ¯ Ready for Bayesian Change Point Modeling:")
    logger.info("   - Data prepared and validated")
    logger.info("   - Log returns computed (stationary)")
    logger.info("   - Volatility patterns understood")
    logger.info("   - Next: Build PyMC model and run MCMC sampling")
    
    logger.info("\n" + "=" * 80)
    logger.info("Core Analysis completed successfully!")
    logger.info("=" * 80)
    
    return {
        'data': data,
        'returns': returns,
        'volatility_results': volatility_results,
        'adf_results': adf_results,
        'returns_adf': returns_adf
    }


if __name__ == "__main__":
    results = main()
